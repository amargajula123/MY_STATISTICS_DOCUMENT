{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c26422a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m15\u001b[0m\n\u001b[1;33m    ex :=\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "What is Statistics :=\n",
    "    Statistics is the science of \"collecting\" , \"Organising\" and \n",
    "    \"Analising\" of the the data.\n",
    "    \n",
    "       you have huze amount of data , you can actully utilise\n",
    "        this particuler data to make sure that there is improve \n",
    "        ment in products bussiness goals some other thing to \n",
    "        better Dession Making \n",
    "        \n",
    "How do i Define data ?\n",
    "\n",
    "        Types of Statistics  :\n",
    "            1.DATA\n",
    "             Data :=\n",
    "                    data is nothing but \"Facts\" or \"Peices\" of \n",
    "                    \"information\" that can be measured .\n",
    "                    \n",
    "\"Example\" :=\n",
    "        the \"height\" of the students in  a class room\n",
    "        {175cm,165cm,143cm,155cm}.\n",
    "        \n",
    "        the \"ages\" of students of a class \n",
    "        {20,30,23,19,18,24,29,}--. Data\n",
    "        \n",
    "        \"Inteligence Quetions\"(IQ) of 5 rondomly selected \n",
    "        individuals like 109,89,129,101,105,106)---->this is also a Data\n",
    "        \n",
    "                \n",
    "Types of \"Statistics\"  :=\n",
    "    Statistics divide into 2 part their are \n",
    "    \n",
    "    1. Descriptive Stats\n",
    "    2. Inferential Stats \n",
    "    \n",
    "    \n",
    "    \n",
    "1.\"Descriptive Stats\" :=\n",
    "            \n",
    "    \"Descriptive Stats\" it consist of \"organising\" and \"summaraising\" the data\n",
    "        \n",
    "    \"ex\" : classroom of \"maths\" students there are 20 people\n",
    "            merks of thre 1st sem\n",
    "            84,86,78,72,75,65,80,81,92,95,97\n",
    "            \n",
    "what kinda IQ you may be getting in \"Descriptive stas\"  \n",
    "\n",
    "        descriptive Stats IQ :  \n",
    "        \n",
    "        what is the \"Avarage\" marks of the students in the class?\n",
    "        \n",
    "        in Descriptive \"Min\" ,\"Median\" ,\"Mode\" like kinda \"questions\" may \n",
    "        come\n",
    "        \n",
    "        \n",
    "what kinda IQ you may be getting in Descriptive sts   \n",
    "        \n",
    "     note := in \"Descriptive stats\" to \"summaraising\" and \"analizing\"\n",
    "             the \"data\" we will be using \n",
    "        \n",
    "        *pdf (probabulity dencity fun)\n",
    "        *Histogram , \n",
    "        *boxplot, \n",
    "        *barchat \n",
    "        *pie chart\n",
    "        \n",
    "        \n",
    "2.Inferential Stats:\n",
    "        \"Inferential Stats\" it is a technique where in we use the \"data\"\n",
    "        that we have measured to form \"conclusions\"\n",
    "        \n",
    "    what may be the inferetial questions\n",
    "        \n",
    "Ex := are the marks of the students of this class room similer \n",
    "      to the age of the maths students cls room in collage ?\n",
    "        \n",
    "                \n",
    "what we do Inferential statistics\n",
    "                \n",
    "    we have \"Sample data\" try to make some kind of \"Inferences\" for\n",
    "    the \"population data\" \n",
    "    \n",
    "    [:.Inferential= అనుమితి ]\n",
    "                \n",
    "note := in this \"Inferential stats\" to \"summaraising\" and \"analize\"\n",
    "       the \"data\" we will be using \"*Hypothesis testing\"  there is\n",
    "        someething called as \n",
    "        \n",
    "        *P_value \n",
    "        *Z_test \n",
    "        *T_test \n",
    "        *Anova_test\n",
    "        *Chi_square_test\n",
    "        \n",
    "                \n",
    "In infrential stats :=\n",
    "    any work that we will be doing 1st of all we \n",
    "    will be considering a sample data based on this \n",
    "    sample data we will be estimating saomething\n",
    "    for the Population data\n",
    "    if i have the sample mean i ll try to estimate \n",
    "    the popution \n",
    "    \n",
    "What is the \"Population\" and \"Sample\" Data ?\n",
    "\n",
    "*\"Populations\" And \"Samples\"  DATA:=\n",
    "    \n",
    "    ex : \"Elections telangana\". \n",
    "        \n",
    "        telangana should have lots of poeple are like \n",
    "        \"population\" its not possible to go and ask to every\n",
    "        one to whom your going vote thats why we take samples\n",
    "        from different different regiouns. and rondamly we can\n",
    "        tell who is most wanted to poeple as a Election Winner\n",
    "        \n",
    "    Note : \"Population\"  basically given or \"Dinoted\" by { \"N\" } \n",
    "           \"Sample\" is basically given or \"Dinoted\" by { \"n\" } \n",
    "            \n",
    "    whenever we work with \"population\" & \"sample\" data this \"sample\"\n",
    "    data \"very much importent\" incase of \"Inferential statistick\" \n",
    "            \n",
    "                \n",
    "                \n",
    "Types of \"Sampling Techniques\" :=\n",
    "    \n",
    "        1.Simple Random Sampling\n",
    "        2.Stratified Sampling\n",
    "        3.Systematic Sampling\n",
    "        4.Convenioun Sampling\n",
    "        \n",
    "1.\"Simple Random Sampling\" :=\n",
    "    \n",
    "    when performing \"Simple Random sampling\" here \"every member\" of \n",
    "    the \"population\" {N} has an equal chance of being selected for \n",
    "    your \"sample\" {n}\n",
    "                \n",
    "NOTE:-   when ever you \"randomly selcting everything\".that is obviously \n",
    "    related to \"every member getting selected\" to in a \"equal propotion\"\n",
    "    there is a chance every item over has the same probabulity. \n",
    "                \n",
    "\n",
    "2.\"Stratified Sampling \":=\n",
    "    \"Stratified Sampling\" is the technique where the \"population\"{N} is\n",
    "    split into \"Non-overlapping\" groups. this is also called as {[Strata]} \n",
    "    \"Strata\" means \"layers\" \n",
    "\n",
    "                                |---->Male\n",
    "                                |\n",
    "                ex:= gender-----| \n",
    "                                |\n",
    "                                |---->Female\n",
    "                            \n",
    "                            |---->(0-18)\n",
    "                            |\n",
    "                ex:=  Age---|---->(18-35)\n",
    "                            |\n",
    "                            |---->(35-60)\n",
    "                            \n",
    "                ex:= Blood Goops ,Tax slabs ,Courses              \n",
    "                        \n",
    "                        \n",
    "3.\"Systematic Sampling\" :-\n",
    "    From the \"population\"{N} we just pick up every \"N`th\" individual \n",
    "    element.           \n",
    "                \n",
    "what is this \"N`th\"  individuals ?\n",
    "        it can be anything that \"you have desided\"\n",
    "                \n",
    " Example :i am out side the mall-->surway(covid)\n",
    "        every 7 th or 8 th person whom ever i \n",
    "        see ill just tell him to do the covide surway. \n",
    "                    \n",
    "4.\"Convenioun Sampling\" :-\n",
    "    in \"Convenioun sampling\" only those people who are intrested will only\n",
    "    be \"participating\".\n",
    "                \n",
    "\n",
    " Example 1 :-\n",
    "    i am doing a surway related to \"data science\" who is person intrest\n",
    "    or has data knowledge on \"data science\" if you concider only those\n",
    "    popeple then it becomes a \"Convenioun Sampling\". \n",
    "                \n",
    " Example 2 :=\n",
    "    surway with repect to \"datascience\" i may only sending surway to the\n",
    "    people who know the AI / datascience \n",
    "                    \n",
    " Example 3 : blind people surway, here obviously blind people are \n",
    "             participate in this surway.\n",
    "        \n",
    "  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "                \n",
    "what are \"Variables\" :=\n",
    "    \n",
    "    a \"Variables\" is a property that can take on any value\n",
    "    \n",
    "    Ex : hight =183, \n",
    "        \n",
    "          {183,123,175,165,134}= list of values\n",
    "        \n",
    "         Weight=45kg , {44,55,67,78,93}= list of values\n",
    "            \n",
    "    Two types of Variables :=\n",
    "                \n",
    "        1.\"Quantitative\" Variables\n",
    "        2.Qualitative or \"Categorical\" Variable\n",
    "            \n",
    "1.Quantitative Variables:=\n",
    "        \"Quantitative Variables\" will have some properties it can be \n",
    "        measured \"Numerically\" we can perform lot of operations\n",
    "        \"Add\",\"Substraction\",\"multiply\",\"divide\",etc\n",
    "            \n",
    "    Ex : Age ,Weight , Height\n",
    "                      \n",
    "2.Qulitative_or_Categorical Variable : =\n",
    "        Based on some \"characteristics\" we can derive some \n",
    "        \"Categorical variables\", here we wont be able to do any\n",
    "        \"Mathematical operations\" its not work b-coz its \n",
    "        \"Categorical Variable\".\n",
    "        \n",
    "                        |--->Male\n",
    "          Ex : Gender---|\n",
    "                        |--->Female\n",
    "          \n",
    "          Ex :blood Groop like A+,B+ ,B-\n",
    "          Ex: T-shirt size Learg ,XL,S,M \n",
    "            \n",
    "            \n",
    "   {IQ} :can we convert a \"Quantitative Variable\" into a\n",
    "         \"Qulitative_or_Categorical\" Variable ANS is yes \n",
    "        \n",
    "          \n",
    "        Ex:=  (0-10)       (10-50)          (50-100)\n",
    "i am            |             |                | \n",
    "concidering     v             v                v\n",
    "this 3 Age    less IQ      Meadium IQ       Good IQ\n",
    "groups     \n",
    "\n",
    "           Ex : Grads in a class A,A+,B,C, grads\n",
    "\n",
    "\n",
    "\n",
    "Quantitative variable :=\n",
    "    \n",
    "    Quantitative will has 2 different types\n",
    "         1.Discrite Variable\n",
    "         2.Continuous Variable\n",
    "            \n",
    "1.Discrite Variable:=\n",
    "    in \"Discrite Variable\" you will spesipicly have a \"whole numbers\" \n",
    "    desimal will not be theire\n",
    "        \n",
    "    Ex :- number of bank account of a person = {1,2,3,4,5}\n",
    "            \n",
    "    Ex :- number of children in a family = {2,3,4,5} you can not say 2.3\n",
    "        children 2.2 children\n",
    "            \n",
    "2.Continuous Variable:=\n",
    "    Continuos data over you can see the \"floating_point\" data or data\n",
    "    basically in a \"continuos manner\".\n",
    "        \n",
    "    whether we are talk about \"hight\" \"weitght\" so this is called \n",
    "    \"Continues data\"\n",
    "    \n",
    "    Ex :- hight   = {172.3cm,185.24cm,165.3cm}like this \n",
    "    Ex :- weitght = {54.5kg,63.2kg}like this\n",
    "    \n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "            \n",
    "            \n",
    "Variable Measurement Scale :=\n",
    "     here we going to understand how do we \"measure variable\" we have 4 \n",
    "    different types of measured variable.\n",
    "    \n",
    "        1.\"Nominal_data\"\n",
    "        2.\"Ordinal_data\"\n",
    "        3.\"Intervall_data\"\n",
    "        4.\"Ratio_data\"\n",
    "        \n",
    "why this \"measured variables\" require ?\n",
    "    your dataset will have this kind of \"variables\" it will have \n",
    "    \"Ratio\",\"Ordinal\",\"Intervall\",\"Nominal\" so that you will be \n",
    "    able to do good data Analysis\n",
    "            \n",
    "1.Nominal_data:=\n",
    "    \"Nominal_data\" these are specifically \"Qulitative_or_Categorical\" data\n",
    "    over here \"Ranking\" is not that importent.\n",
    "        \n",
    "    Ex : colour\n",
    "    Ex : flowes\n",
    "    Ex : Gender\n",
    "            \n",
    "2.Ordinal_data:=\n",
    "    this is obviously \"qualitative_or_categorical\" data the \"order\" of\n",
    "    the \"data\" matters but value does not like we can assing \"ranks\" to it\n",
    "        \n",
    "        ex : i have 4 students\n",
    "               marks        ranks\n",
    "                100          1st---> it may not be numeric but rancking\n",
    "                96           2nd     is important\n",
    "                56           3rd\n",
    "                43           4th ; here rank is \"ordinal data\"\n",
    "                                we are not wooried what marks that person\n",
    "                                got we just have a rank that is \"ordinal data\"\n",
    "                                \n",
    "3.Intervall_data:=\n",
    "    in Intervall_data the order matters and value is\n",
    "        also matters\n",
    "        \n",
    "        ex : Teparature\n",
    "            Faranheits \n",
    "            \n",
    "            70-to-80  80-90 90-100\n",
    "        you have some range of values b/n them this called\n",
    "        Intervall_data\n",
    "        \n",
    "        hear we use \"histograme\"  \n",
    "                        \n",
    "                \n",
    "        \n",
    "4.Ratio_data:=\n",
    "        \n",
    "        \n",
    "#####################################################################\n",
    "\n",
    "1. Nominal: Categorical data with no inherent order or ranking. Examples:\n",
    "    - Colors (red, blue, green)\n",
    "    - Genders (male, female)\n",
    "    - Nationalities (American, Indian, Chinese)\n",
    "2. Ordinal: Categorical data with a natural order or ranking, but no equal\n",
    "    intervals between consecutive levels. Examples:\n",
    "    - Education level (high school, bachelor's, master's, PhD)\n",
    "    - Satisfaction ratings (unsatisfied, neutral, satisfied)\n",
    "    - Movie ratings (1-5 stars)\n",
    "3. Interval: Numerical data with equal intervals between consecutive levels,\n",
    "    but no true zero point. Examples:\n",
    "    - Temperature (Celsius or Fahrenheit)\n",
    "    - IQ scores\n",
    "    - Years of education\n",
    "4. Ratio: Numerical data with equal intervals between consecutive levels\n",
    "    and a true zero point. Examples:\n",
    "    - Height\n",
    "    - Weight\n",
    "    - Age\n",
    "\n",
    "@Continuous Variables\n",
    "\n",
    "1. 'Interval': Temperature, IQ scores, years of education (can take any value within a range)\n",
    "2. 'Ratio': Height, weight, age (can take any value within a range, with a true zero point)\n",
    "\n",
    "@Discrete Variables\n",
    "\n",
    "1. 'Nominal': Colors, genders, nationalities (categorical, distinct groups)\n",
    "2. 'Ordinal': Education level, satisfaction ratings, movie ratings \n",
    "    (categorical, ordered but not continuous)\n",
    "\n",
    "In summary:\n",
    "\n",
    "- Continuous variables: Interval and Ratio\n",
    "- Discrete variables: Nominal and Ordinal\n",
    "\n",
    "###################################################################\n",
    "        \n",
    "        \n",
    "Frequency Distribution : =\n",
    "    \n",
    "    sample_data_set ={rose,lilly,sun_flower,rose,lilly,sun_flower\n",
    "                     rose,lilly,lilly..... }\n",
    "    \n",
    "    \n",
    "    flower      Frequency             * Cummilative_Frequency\n",
    "    -------     ---------                        3\n",
    "    rose             3                           7 \n",
    "    lilly            4                           9\n",
    "     sun_flower      2             the frequency getting added all\n",
    "                                   this is frequency distribution\n",
    "                                   table from this table you can\n",
    "                                   derive bar Graph,pie chart\n",
    "                                   you can derive different \n",
    "                                   different things \n",
    "here we use \"bar chat/graph\" to \n",
    "summarising the data  \n",
    "\n",
    "if the variable is \"continous\" if that point of  we can draw\n",
    "\"histogram\"\n",
    "\n",
    "if the variable is \"discrite\" we can definatly draw \"bar chart/graph\"\n",
    "\n",
    "\n",
    "Bar chart :=\n",
    "    ^\n",
    "    |\n",
    "    |\n",
    "  4---              |----|             \n",
    "    |               |    |\n",
    "  3-- |----|        |    |\n",
    "    | |    |        |    |          \n",
    "  2---|    |        |    |     |----|\n",
    "    | |    |        |    |     |    |\n",
    "  1---|    |        |    |     |    |\n",
    "    |----|------------|----------|---->\n",
    "        rose        lilly      sunflower\n",
    "        \n",
    "        \n",
    "\"Histograms\" :-\n",
    "    \n",
    "    here data should be \"continuous\" it can be discrit\n",
    "    continuous it can not be discrite continuous\n",
    "    \n",
    "    Ages =  {10,12,14,18,24,26,30,35,36,37,40,41,42,43,50,51}\n",
    "    \n",
    "    Ages is Continous values like 23.5 years old 22.6 years old like that\n",
    "   \n",
    "in \"histogram\" we make \"something\" called as \"BINS \" by defult \"bin\" \n",
    "size is 10 \n",
    "  \n",
    " frequency nothing but\n",
    "total No.of count\n",
    "    ^\n",
    "    |\n",
    " 4----     |----|    |----|----|\n",
    "    |      |    |    |    |    |\n",
    " 3----     |    |    |    |    |\n",
    "    |      |    |    |    |    | \n",
    " 2----     |    |----|    |    |----| \n",
    "    |      |    |    |    |    |    |\n",
    " 1----     |    |    |    |    |    |\n",
    "    |------|----|----|----|----|----|----|----|-->\n",
    "    0      10   20   30   40   50   60   70   80\n",
    "        \n",
    "PDF_(Probability_dencity_Function) :=\n",
    "    we just take \"histogram\" and we are going to \"smoothen\" it after \n",
    "    soomethening that is some thing called  \"Probability_dencity function\"\n",
    "                        \n",
    "how do we \"smmothen\" \"histogrms\" ?\n",
    "   there is amezing functionality which is called as \"Kernal_dencity Estimater\" {KDE} \n",
    "    which helps you to \"smoothen\" the \"histogram\"\n",
    "    \n",
    "    ex : sns.distplot(df[\"sepal_lenght\"])\n",
    "                    \n",
    "{IQ} what is \"pdf\" function ?\n",
    "    it is a \"smoothen version\" of a \"Histogram\"\n",
    "note :=\n",
    "    \"histogram\" should be a 'continueous' if it is not \"continuous\" it \n",
    "    will be no use\n",
    "    \n",
    "note :=\n",
    "    \"kernal dencity estimater\" work is to \"smoothen\" the \"histogram\"\n",
    "            \n",
    "why do we use \"BAR\" graph and \"Histogram\"\n",
    "\n",
    "\"BAR\" is specally used for \"descrite\" variables\n",
    "\"Histogram\" is specally used for \"Continuous\" variables\n",
    "\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@                \n",
    "        \n",
    "the entire  statistics with respect to data-science is\n",
    "devided into to types \n",
    "\n",
    "     1), \"Descriptive Stats\"\n",
    "     2), \"Inferential Stats\"\n",
    "\n",
    "@@@@@@@@@@@@@@@@@@@@@@      \n",
    " *Descriptive Stats :=\n",
    "@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "    in descriptive Stats we have two things\n",
    "    \n",
    "         1. Measure of Central tendency \n",
    "         2. Measure of Disperson, \n",
    "              any thig that is related to\n",
    "            summaraising  the data. \n",
    "            ---> here we r going to understand \"plots\" like hist\n",
    "            pdf , cdf , what techniqes we use  we r going to\n",
    "            understand . Probability , Permutation ,mean,median\n",
    "            Mode, Variance ,Derivation\n",
    "            \n",
    "All Distributions :\n",
    "          1.Normal or goussion distribution\n",
    "          2.Standard normal distribution\n",
    "          3.Z-Score\n",
    "          4.Log Normal Distribution\n",
    "          5.Bernoulli`s distribution\n",
    "          6.Binomial distribution\n",
    "          7.Pareto (or) Power law Distribution\n",
    "          8.Trassformation and Standardization\n",
    "          9.Q-Q plot.\n",
    "        \n",
    "@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "  *Inferential Stats :=\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "       here we going to understand.\n",
    "          1. Z-test --.there are multyple ways\n",
    "                        to execute z test\n",
    "          2. T-test\n",
    "          3. Anova test (or) F-test \n",
    "          4. Chisqure_test\n",
    "          5. Hypothesis_testing :=here we going to understand \n",
    "                                  {p} value one imp thing is \n",
    "                                  called *{Confidence Interwalls}\n",
    "                        \n",
    "          6.we can learn how to see Z-table,T-table, Chisqure_table\n",
    "##########################################################################       \n",
    "different Distibutions\n",
    "\n",
    "Distribution :=\n",
    "        \"Distribution\" main perpose is to different different destribution \n",
    "        are there we can basically have some idia about the \"DATASET\" \n",
    "    \n",
    "    #########################################\n",
    "    **1. Normal or Goussion distribution**\n",
    "#######################################\n",
    "\n",
    "ex of \"Normal or Goussion distribution \"\n",
    "is 68.2% - 95.4% - 99.7% rule\n",
    "\n",
    "                    *  *  *\n",
    "                 *     |     *\n",
    "               *       |       * ---->Bell curv     \n",
    "            *          |         *\n",
    "          *            |           *\n",
    "        *      -1_SD   |  SD_1      *\n",
    "       ------------|---|---|------------\n",
    "                       |-- it can be \"Mean\"/\"Median\"/\"Mode\"\n",
    "\n",
    "whenever see \"Bell_Curve\" shap that is called \"Normal or goussion\" \n",
    "\"distribution\".\n",
    "        \n",
    "most imp thing we have to know some Properties the total \"Bell_Curve\"\n",
    "right part == left side part\n",
    "\n",
    "                  \"Bell_Curve\" = 0.5 + 0.5 = 1\n",
    "            \n",
    "1. 'Emparical_rule' :=\n",
    "    \n",
    "            68.2% - 95.4% - 99.7%\n",
    "        \n",
    "{IQ} how many elements from a dataset will be present if they follow\n",
    "\"Normal or Goussion\" Bell_Curve ? \n",
    "\n",
    "1.ANS :=\n",
    "    with in the \"normal or goussian\"  Bell_Curve with in the \"1st\" SD\n",
    "    around 68.2% of the dataset will be folling\n",
    "    \n",
    "2.Ans :=\n",
    "    with in the \"normal or goussian\"  Bell_Curve with in the \"2nd\" SD\n",
    "    around 95.4% of the dataset will be folling \n",
    "         \n",
    "3.Ans :=\n",
    "    with in the \"normal or goussian\" Bell_Curve with in the \"3rd\" SD \n",
    "    around 97.7% of the dataset will be folling   \n",
    "                \n",
    "68.2% - 95.4% - 99.7% this is called as Empirical role\n",
    "                   \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "      {g}3:17:22                \n",
    "                    \n",
    "            \n",
    "          <-----------3.SD-99.7%-------------> \n",
    "               <--------2.SD-95.4%-------->\n",
    "                    <---1.SD-68.2%--->\n",
    "        __|____|____|________|_______|____|____|__\n",
    "          µ    µ    µ        µ       µ    µ    µ\n",
    "          -    -   -   <---1.SD--->  +    +    +  \n",
    "          3σ   2σ  1σ                1σ   2σ   3σ\n",
    "            \n",
    "     \n",
    "Where your colculating the \"lower fence\"& \"heigher fence\" why did we use\n",
    "\"1.5 (IQR)\" so that atleast to same level it should be  able to cover  the \n",
    "\"entire data \"  \n",
    "    \n",
    "*one another \"rule\" to find out \"outliers\" :=\n",
    "Ans:=\n",
    "    anything that falls  away from  '3rd' \"Standard diviation\" we can \n",
    "    consider as a \"outliers\"\n",
    "        \n",
    "        \n",
    "imp_point := \n",
    "    \n",
    "which all kind of dataset will fall into this kind of curve ?\n",
    "Ans:=\n",
    "    Iris DATA_set {petallenth,sepallenth,petalwidth,sepalwidth}\n",
    "                \n",
    "  Ex : the weight of human follows \"goussion distribution\"\n",
    "  Ex : height follow the \"goussion distribution\"\n",
    "    \n",
    "we have a tool whether Distribution follows  \"goussion distribution\" or\n",
    "not to check this we have Q-Q plot\n",
    "                \n",
    "                 \n",
    "####################################        \n",
    "**2. Standard normal distribution**\n",
    "#################################### \n",
    "\n",
    " DATA = {1,2,3,4,5}   mean µ = 3\n",
    "    assume     SD  σ = 1.414 let just concider σ ~ = 1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "                                      here µ = 3\n",
    "    -|----|----|----|----|----|----|- SD  σ = 1.414 let just concider = 1\n",
    "     0    1    2    3    4    5    6\n",
    "                    µ\n",
    "            \n",
    "convert this in such a way 'µ' = 0 & SD 'σ' = 1 can i convert some dataset\n",
    "into a which has 'µ' = 0  SD 'σ' = 1 for this proces\n",
    "        \n",
    " Note :\n",
    "    how many SD to the right and left need to find out so we can use\n",
    "    Z-score table\n",
    "    \n",
    "        \n",
    "simple formula:=\n",
    "            \n",
    "      DATA = {1,2,3,4,5}      \n",
    "    \n",
    "                           xi - µ\n",
    "                Z-score = --------\n",
    "                             σ\n",
    "                \n",
    "                       1-3\n",
    "                    = ----- =-2\n",
    "                        1\n",
    "                        \n",
    "     2-3            3-3         4-3           5-3        6-3    \n",
    "   =----- = -1 , = ----- = 0, =----- = 1 , = ----- = 2 ,----- = 3\n",
    "      1              1           1             1          1\n",
    "    \n",
    "                  *\n",
    "              *        *\n",
    "           *              * \n",
    "        *                    *\n",
    "      *                         *\n",
    "    *                             *   \n",
    "  --|----|----|----|----|----|----|--  here µ = 3 has got converted to\n",
    "   -3   -2   -1    0    1    2    3    µ = 0  & σ = 1  we have converted \n",
    "                                       the distribution into the \n",
    "                                       \"Standard normal distribution\" \n",
    "     \n",
    "Why we have to caonver this  \"µ = 0\"  & \"σ = 1\"  ?\n",
    "\n",
    "Ans:=\n",
    "    There is Concept of \"Standardization\"  Vs  \"Normalization\",\n",
    "    \n",
    "            \n",
    "            \n",
    "    { IQ } Stats inerview Q\n",
    "What percntage of svore falls above 4.25\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  --|----|----|-----|-----|-----|----|--\n",
    "    1    2    3     4     5     6    7\n",
    "    \n",
    " here we use z score test   \n",
    "    *****************\n",
    "    3. Z-Score formula\n",
    "****************    \n",
    "    \n",
    "                           xi - µ\n",
    "                Z-score = --------\n",
    "                             σ\n",
    "                \n",
    "                           4.25 - 4\n",
    "                Z-score = ---------- = 0.25 = \n",
    "                              1 \n",
    "                    \n",
    "                    see the Z crore left table value 0.5987\n",
    "                    \n",
    "                      Mean(µ) to 1st SD = 0.987\n",
    "                        \n",
    "                        0.5 - 0.987 = -0.487\n",
    "                        1 - 0.987   =  0.013\n",
    "                        \n",
    "                        \n",
    "                    1 -  0.5987              = 0.4013  \n",
    "                    1 - left area body curve = 0.4013 means 40%\n",
    "                         0.5987 + 0.4013 = 1\n",
    "                        \n",
    "                      0.5 + 0.25 = 0.75\n",
    "              \n",
    "  \n",
    "        \n",
    "(imp) 1. \"Standardization\"  Vs  \"Normalization\" :-\n",
    "\n",
    "Ans:-\n",
    "    Standardization will help you to scale down your features  \n",
    "   based on standard normal distribution \n",
    "   when ever we talk about @Standardization inshot internally\n",
    "there is a \"Z-score formula\" getting applyed here we are \n",
    "getting converted as mean { µ = 0 } SD { σ = 1}\n",
    " \n",
    "        years       kgs        rupees\n",
    "        age        weight      salary\n",
    "        25           65          30k\n",
    "        26           53          40k   in this \"dataset\" every feature has \n",
    "        28           73          60k   a different \"Units\"(year,kgs,rupess)\n",
    "        30           60          50k   so its calculated based on different\n",
    "        32           70          80k   unit.\n",
    "        \n",
    "        mean = 28.2\n",
    "        \n",
    "        \n",
    "                           xi - µ\n",
    "                Z-score = --------\n",
    "                             σ\n",
    "                                  *---> -1.25 fall in this resion\n",
    "        25 - 28.2\n",
    "        ---------- = -1.25   ---|-.--|------\n",
    "          2.56                 -1   0  \n",
    "            \n",
    "            \n",
    "      -3.2 ,-2.2,-0.2,1.8,3.8     \n",
    "            \n",
    "\n",
    "(V.V.IMP)\n",
    "can we bring all this futures(\"age\",\"weight\",\"salary\") into the \"same unit\n",
    "scale\" ?\n",
    "\n",
    "Ans:-why we need to bring to the \"same unit scale\" is that for the \n",
    "    \"ML Algorithem\" what ever maths gets applyd quickly caluculate. \n",
    "    b-caoz all the values are in the \"same unit scale\",\n",
    "    \n",
    "    for this process we use a technique called as 'Standardization' \n",
    "    in this we apply 'Z-score'.\n",
    "    \n",
    "Note:=\n",
    "    in 'Standardization' we apply 'Z-score' with each and every \"feature\" \n",
    "    \n",
    "2.Normalization :=\n",
    "    normalization gives  you a proces where you can define \"lower bound\"\n",
    "    & \"upper bound\" and  you can convert the data B/W them \n",
    "\n",
    "in 'Normalization' we specify \"min max scaller\",in \"min max scaler\" you\n",
    "specify \"min value\" and \"max value\", it applys a formula the \"entire future\" in \n",
    "such a way that all values will be converted B/W \n",
    "        max and min values \n",
    "        \n",
    "suppose want to shift the \"Entire values\" B/W 0 to 1,2,3,4..you set like\n",
    "\"min value\" 0 \"max value\" = 1,2,3,4...something its posible in \"Normalization\",\n",
    "we specify [ min max Scaler ]\n",
    "        \n",
    "\n",
    "when should we use 'Normalization' & 'Standardization' ?\n",
    "\n",
    "Ans:=\n",
    "    in most of the 'ML usecases'  we  goes to use with \"Standardization\"\n",
    "       [ from sklearn.preprocessing import StandardScaler ]\n",
    "        \n",
    "    \"Normalization\" is specifically learn in \"CNN\" which we going to \n",
    "    learn in  \"DL\" Image traning object detection.\n",
    "    \n",
    "    in \"CNN\" (Convolutional Neural Network) what ever picshals we having\n",
    "    in images we have like picshals will ranging b/w 0 to 255 so we take\n",
    "    this picshals we try to \"Normalize\" it B/w [ 0  to 1 ]\n",
    "    \n",
    "\n",
    "Where we you feature sca        \n",
    "some of the algorithems like \"KNN, k means clustering\" and wher 'Gradient'\n",
    "desent curv when you need to find the 'gobal minima' that particuler point\n",
    "if you have parabola curv get that gobal point marge quckly we basically\n",
    "have to \"scal down\" that particuler point some of the algorithems like\n",
    "Dession tree random forest xg_boost technics bagging technic all dession tree\n",
    "you dont need to scal down your values \n",
    "\n",
    "\n",
    "        \"Normalization\" is specifically learn in CNN which we\n",
    "        going to learn in  DL Image traning object detection\n",
    "        \n",
    "        \n",
    "        ex future 1      Normalization         F`  \n",
    "          2                                   0.14\n",
    "          5                     X-X min       0.571 \n",
    "          6            Xnor  = ---------      0.71\n",
    "          8                    Xmax-Xmin        1  \n",
    "          1                                     0\n",
    "            \n",
    "            the upper normalization formula used for \n",
    "            convert that features into 0 - to - 1 this \n",
    "            formula is basically called as \"min max scaler\" \n",
    "               \n",
    "  2-1           5-1           6-1         8-1     1-1\n",
    "  --- = 0.14  ,-----=0.571 , -----=0.71, ----=1, ----- = 0\n",
    "  8-1           8-1           8-1         8-1     8-1\n",
    "\n",
    "after applying formula the values ranging B/W 0 to 1\n",
    "   values = 0.14 , 0.57 , 0.71 , 1 , 0\n",
    "    \n",
    "NOte  :=  \n",
    "    when you see \"Bell curve\" Graph you to remember\n",
    "    goussian/Normal Distribution  \"Emparical_Formula\"\n",
    "    68.2% - 95.4% - 99.7% rule\n",
    "                             \n",
    "    \n",
    "   ###################################\n",
    "   **4. Log Normal Distribution**\n",
    "#############################\n",
    "   in \"Log Normal Distribution\" we are going to see \"Skuwed Curves\"\n",
    "   means they have \"lot of outliers\" when u see \"Skuwed Curves\" graph\n",
    "   that is called a 'log normal distribution'\n",
    "    \n",
    "    what ever DATA after you create a histogame and smoothen\n",
    "    it if that is looks like \"Skuwed curve\" that is something \n",
    "    called as \"Log normal dstribution\" \n",
    "\n",
    "    \n",
    "                    *  *  *\n",
    "                 *           *\n",
    "               *                * ---->Bell curv     \n",
    "            *                     *\n",
    "          *                        *\n",
    "        *      -1_SD      SD_1      *\n",
    "       ------------|---|---|------------\n",
    "                       |-- it can be \"Mean\"/\"Median\"/\"Mode\"\n",
    "            \n",
    "            \n",
    "      Ex : lenth of the comments\n",
    "      Ex := wealth of the person     \n",
    "            \n",
    "            \n",
    "      {IQ} can we convert this \"log normal dstribution\" into\n",
    "           \"Goussian/Normal distribution\" like \"Bell curve\" ?\n",
    "        \n",
    "        Ans := Yes \n",
    "            \n",
    "            \n",
    "            {g}bell curve\n",
    "            \n",
    "            X = Log normal Distribution it lokks \"Skuwed_curve\"\n",
    "               \n",
    "\"X=exp(y) or Y =log(x)\"  then this will follow \n",
    "                       a \"Bell curve\" or Normal \n",
    "                       or goussion distribution \n",
    "        x      y=log(x)\n",
    "        25       --\n",
    "        30       --   --> this values will become \"bell curve\"{g}\n",
    "        45       --       or \"goussion/normal distribution\"\n",
    "        \n",
    "\"X\" = \"log normal distribution\" that looks \"skuwed curv\" \n",
    "\n",
    "\n",
    "when we apply \"Y = log(X) / X=exp(y)\" then \"log normal distribution\"\n",
    "will follow a \"goussion distribution\"\n",
    "\n",
    "                    log-->log10 \n",
    "                    ln --->lne\n",
    "to transefor 'log narmal distribution' to 'normal'\n",
    "or 'goussin distribution' the link is  below \n",
    "\n",
    "link := https://www.youtube.com/watch?v=3gfhbXt9TcQ \n",
    "        \n",
    "    #################################        \n",
    "    **5. Bernoulli`s distribution**           \n",
    "################################  \n",
    "\n",
    "    when ever we talk about \"Bornoulli distribution\" lets \n",
    "    say that we talking about \"cotegirical values\" here \n",
    "    many things works with \"probabilitys\" it has \"binary ouputs\". \n",
    "    \n",
    "    \"Bornoulli distribution\" works with \"probability distribution\"\n",
    "    with respect to probability here ou have \"P & Q \"values\n",
    "    P , Q \n",
    "    Q = 1-P\n",
    "    \n",
    "what is Probability density fun and probability mass fun?\n",
    "\n",
    "when ever we trying to plot for a \"categorical varible\" that\n",
    "point of time we say basically it as \"probability mass fun\"\n",
    "\n",
    "\"probability mass fun\" will looks formula\n",
    "               if K = 0 so Q = 1-P\n",
    "                  K = 1 so P\n",
    "                        \n",
    "\n",
    "when ever we trying to plot for a \"continous varible\" that\n",
    "point of time we say basically it as \"probability mdencity fun\" \n",
    "\n",
    "    Ex :in bernoulli distribution there are only 2\n",
    "        outcomes means it can be iether 0 or 1\n",
    "        \n",
    "        tosing a coin \n",
    "        what is the probability of \"head\" and \"tail\"\n",
    "        probability head =0.6 tail=0.4\n",
    "        \n",
    "        per suppose i have a fair coin :\n",
    "            probabulity of head\n",
    "            \n",
    "            tossing a coin  # this is only a single try  \n",
    "        \n",
    "              P_r(H)=0.5    q=1-P_r(H) = 1-0.5 = 0.5\n",
    "            \n",
    "        per suppose i dont have a fair coin :    \n",
    "        \n",
    "        tossing a coin   # this is only a single try  \n",
    "        \n",
    "              P_r(H)=0.3    q=1-P_r(H) = 1-0.3 = 0.7\n",
    "            \n",
    " IQ : when we combine multiple \"Bernoulli`s distribution\" \n",
    "    let say  Example tosing a coin  i ll be having P,Q\n",
    "    in every expiriment\n",
    "       per suppose 5 defferent expiriment \n",
    "         \n",
    "    {P or Q}  {P or Q}  {P or Q}  {P or Q}  {P or Q}->if i \n",
    "    \"combine\" entire data this will be my \"binomial destribution\"\n",
    "    if i just concider \"only one\" that will be \"Bernoully destribution\"\n",
    "    \n",
    "        \n",
    "     \n",
    "    Probablit\n",
    "    \n",
    "            ^\n",
    "        1 --|--\n",
    "            |                           this bassically called as \n",
    "       0.8--|--                       | probabulity mass function\n",
    "            |                         |\n",
    "       0.6--|--                       |\n",
    "            |                         |\n",
    "       0.4--|--                       | \n",
    "            |          |         |    |\n",
    "       0.2--|--   |    |         |    |\n",
    "            |_____|____|_________|____|______>\n",
    "                  |    |         |    |\n",
    "                 h=2  H=5      T=5  T=8\n",
    "            \n",
    "         Formula :=   \n",
    "            q = 1-p if k = 0\n",
    "            p       if k = 1\n",
    "        \n",
    "            pκ(1-p)κ-1\n",
    "            \n",
    "            most of the \"binary output\" clasification\n",
    "            where ever we have we are basically say that\n",
    "            as a\"bernoulli distribution\" \n",
    "            \n",
    "    ##############################       \n",
    "    **6. Binomial distribution**          \n",
    "#############################  \n",
    "\n",
    "  which is Extenction of \"Bernoulli\" that is called\n",
    "  as  \"Binomial distribution\" in this you have\n",
    "  \"combination\" of \"Bernoulli distributions\"\n",
    "    \n",
    "    leman way  with respect to every trail they will be a\n",
    "    benoulli distribution here we have multyple trail\n",
    "    \n",
    "    Binomial distribution is given with 2 values \"B(n,p)\"\n",
    "    n = \n",
    "    p = probability\n",
    "    here we denote as B(n,p)\n",
    "    \n",
    "       n----> how many expiriments are their or trails\n",
    "       p----> what is the probabulity\n",
    "    \n",
    "    Ex : 5 times  Tosing a coin\n",
    "        \n",
    "     for 1 time \n",
    "        probabulity p , q i will be getting \n",
    "     for 2 time\n",
    "        probabulity p , q i will be getting       \n",
    "     for 3 time   \n",
    "        probabulity p , q i will be getting   \n",
    "     for 4 time\n",
    "        probabulity p , q i will be getting               \n",
    "     for 5 time \n",
    "        probabulity p , q i will be getting \n",
    "        \n",
    "        B-coz our data set is binari classification \n",
    "        dataset ether 0 or 1 only\n",
    "        \n",
    "        note : binary bassically means 2 outputs \n",
    "       \n",
    "    Fomula:=\n",
    "            (n)\n",
    "            ( ) pk qn−k\n",
    "            (k)\n",
    "            \n",
    "            \n",
    "    ###########################################        \n",
    "    **7. Pareto (or) Power law Distribution**            \n",
    "########################################## \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "{g}\n",
    "\n",
    "  \"Power_law\" Distribution also called as \"Pareto distribution\"\n",
    "  this is a \"non goussian/normal distribution\". Power law Distribution\n",
    "  says 80-20% Rule,  here  this graph is extection of \"log Normal\n",
    "  distribution\"  graph \n",
    "\n",
    "\n",
    "       Ex : 20% of the team is resposible for winning 80%\n",
    "          of the match\n",
    "            \n",
    "        Ex : 20% of the entire nation hold 80% of the oils\n",
    "            \n",
    "        Ex : 80% of the company project are done by 20% of the\n",
    "             people in a team\n",
    "                \n",
    "        Ex : 80% of sales is done by the 20% of the famous product\n",
    "            \n",
    "to transefor 'log narmal distribution' to 'normal'\n",
    "or 'goussin distribution' the link is  below \n",
    "\n",
    "https://www.youtube.com/watch?v=3gfhbXt9TcQ\n",
    "####################################################################            \n",
    "  \n",
    "    \"Descriptive Stats\"\n",
    "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "   *Measure of Central tendency:=\n",
    "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
    "\n",
    "it refers to the measure used to determine the \n",
    "center of the distribution dataSET\n",
    "\n",
    "  \"Central tendency\" this basically identyfies \"central position of the\"\n",
    "  \"dataset\".so we need to use \"mean median mode\"\n",
    "    \n",
    "    here we learn 3 things\n",
    "          1. Mean\n",
    "          2. Median  \n",
    "          3. Mode\n",
    "        \n",
    "                    *  *  *\n",
    "                 *           *\n",
    "               *                * ---->Bell curv     \n",
    "            *                     *\n",
    "          *                        *\n",
    "        *                            *\n",
    "        ---------------|----------------\n",
    "                       |-- it can be Mean/Median/Mode\n",
    "            \n",
    "            Mean/Median/Mode := \"dertermines\" the \"curve\" of he \"top\"\n",
    "            \n",
    "            ex =\n",
    "what is mean, mode, median, and when we use this in EDA and\n",
    "FUTURE Engineering ?\n",
    "\n",
    "@@@@@@@@@@\n",
    "***MEAN***\n",
    "@@@@@@@@@@\n",
    "Population{N} mean(µ):= {\"µ\"}\n",
    "    wheneve we are discussing about \"mean\" you need to remember we are \n",
    "    trying to find out the \"average\" of a \"specific distribution\" \"mean\".\n",
    "    is the part of \"measure of central tendency\" \n",
    "    \n",
    "randaom values X={1,1,2,2,3,3,4,5,5,6} just concider this is\n",
    "population.\n",
    "\n",
    "note: \"population mean\" given by symbol(\"µ\") \n",
    "     Xi itereats the \"data set\" \n",
    "                                     \n",
    "                     \n",
    "                     N   Xi             {G}\n",
    "                µ  = ∑   ---\n",
    "                    i=1   N\n",
    "                    \n",
    "                    \n",
    "               1+1+2+2+3+3+4+5+5+6      32\n",
    "            µ= --------------------  = ----  = 3.2 this is \"average\"\n",
    "                       10                10\n",
    "            \n",
    "            ---> so central element is 3.2\n",
    "            \n",
    "            if i plot PDF distribution my bell curv\n",
    "            my \"central element\" is 3.2\n",
    "            \n",
    "if take this 3.2 value show it in Distribution it will be like\n",
    "            \n",
    "                    *  *  *\n",
    "                 *           *\n",
    "               *                * ---->Bell curv     \n",
    "            *                     *\n",
    "          *                        *\n",
    "        *                            *\n",
    "        ---------------|----------------\n",
    "                       |-- it can be \"Mean\" now\n",
    "                      3.2\n",
    "                \n",
    "                \n",
    "                   n    Xi\n",
    "sample mean(\"x̄\") = ∑  ---- = 3.2 \n",
    "                  i=1   n\n",
    "    \n",
    "    \n",
    "@@@@@@@@@@@@            \n",
    "***MEDIAN***\n",
    "@@@@@@@@@@@@@\n",
    "\n",
    "Sample{n} mean(x̄) := {\"x̄\"}\n",
    "\n",
    "X={1,2,2,3,4,5}\n",
    "\n",
    "\n",
    "note: \"Smaple mean\" given by symbol= ( \"x̄\" )\n",
    "    \n",
    "                     n    xi\n",
    "                x̄  = ∑   ---\n",
    "                    i=1   n\n",
    "                \n",
    "                   1+2+2+3+4+5      17  \n",
    "                x̄ =------------- = --- = 2.83\n",
    "                       6            6\n",
    "                    \n",
    "               per suppose add  some out_lier like 100\n",
    "Oulier :=\n",
    "    A \"outlier\" is an element that look completly different then that of \n",
    "    the \"Entire Distribution\" is called an \"outliers\"\n",
    "    \n",
    "            \n",
    "                    1+2+2+3+4+5+100     117  \n",
    "                x̄ =---------------- =   --- = 16.71\n",
    "                           7             7 \n",
    "                    \n",
    "     \n",
    "    x̄ = 2.83   x̄ = 16.71 there is very huze difference is occuring so \n",
    "    something we do for this spesific \"outliers\" it should not afect \n",
    "    this distribution. so for that we use \"median\"\n",
    "    \n",
    " @Median :\n",
    "    \n",
    "    \n",
    "    X = 1,2,2,3,4,5,100\n",
    "    \n",
    "    in median 1st of all you \"sort the numbers\" pick up the \n",
    "    central elements\n",
    "    \n",
    "    {2,2,1,3,5,4,100} sort the numbers --->{1,2,2,3,4,5,100}\n",
    "    \n",
    "            x=(1,2,2,3,4,5) find median we r no add 100 here \n",
    "        \n",
    "2,3 midile 2 numbers      2+3\n",
    "                     x = ----- = 2.5--> \"Median\" \n",
    "                           2\n",
    "        \n",
    "so here 2.5 ~ 2.83, 2.5 is apractumatly equal or close to 2.83(mean) \n",
    "with help of \"median\" we got 2.5 with the help of \"mean\" we got 2.83\n",
    "            \n",
    "x=(1,2,2,3,4,5,100) just by adding an outlier my \"median\"=3 the so\n",
    "\"median\" defference hardly moved from 2.5 to 3\n",
    "                \n",
    "here by, adding an \"outliers\" , \"mean\" moment move from 2.83 to 16.71. \n",
    "but when comes to \"meadian\" just by adding an \"outliers\" \n",
    "it hardly move from 2.5 to 3, so \"Median\" definatly work in the \n",
    "case  of an \"Outliers\",so our distribution will not get effected\n",
    "                        \n",
    "Median  x = 3  \n",
    "       { Median Works well with outliers}\n",
    "    \n",
    "    \n",
    "@@@@@@@@@@@        \n",
    "***MODE***\n",
    "@@@@@@@@@@@\n",
    "\n",
    "mode is nothing but which  element has the \"highest\" \n",
    "\"frequency\" and we going to take that element\n",
    "\n",
    "   X = {1,2,2,3,3,3,4,5,6,6,7}\n",
    "    \n",
    "    MODE is X = {3} b-coz it repeted 3 times more then all \n",
    "\n",
    "    Disadvantage :=  \n",
    "        \n",
    "    Y = {1,2,2,3,3,4, 4,5,6} give error this happenss in 'python 2.7'\n",
    "\n",
    "    but now\n",
    "    MODE is Y = {2,3,4} when ever you see this kind senario just go to \n",
    "    \"midian\" \n",
    "    \n",
    "    Example MODE := a girl went to market she picked up flowers\n",
    "            like --> lilly,sunflower, rose,  .... , ....she pickd up \n",
    "            some flowers she did not labled it this unlabled flowers\n",
    "            will be \"Nan\" values.\n",
    "            with the help of \"mode\" the missing value will be replce \n",
    "            wich ever is the most freaquent picked up flower that\n",
    "            can actually replace in the place of Nan so' Nan value\n",
    "            'will be replaced with  most frequent occuring Element'    \n",
    "    \n",
    "    \n",
    "    where exactly MODE can be help full ?\n",
    "    \n",
    "    in EDA \"Future Engineering\" my data set has 'NAN values' inside if \n",
    "    that data_set \"continuous_values\" we replace this \"NAN\" values with\n",
    "    the help of {*MEAN}, if the data_set has \"OUTLIERS\" so replace this \n",
    "    \"NAN\" values with the help of  {*MEDIAN}\n",
    "    \n",
    "    if it just \"continuous_values\" use \"Mean\"\n",
    "    if it hase \"Outliers\" use \"Median\"\n",
    "    \n",
    "    ok when should we use MODE ?\n",
    "     you have a \"Categorical variable\" and suppose you have to replace\n",
    "     NAN values something then you definetly use {*MODE}\n",
    "        \n",
    "  \"Descriptive Stats\"\n",
    "$$$$$$$$$$$$$$$$$$$$$$$$$$$             \n",
    "   *Measure of Disperson :=\n",
    "$$$$$$$$$$$$$$$$$$$$$$$$$$$ \n",
    "    \n",
    "    here we talk 2 types of Disperson\n",
    "    \n",
    "       1. Variance\n",
    "       2. Standard diviation\n",
    "    \n",
    "what exactly the \"Disperson\" means ?\n",
    "\n",
    "this \"Disperson\" can be also called as something like \n",
    "\"Spreadness of the data\" how the data is spread.\n",
    "\n",
    "to Understand the \"Spredness\" of \"Data\" we need to Understand \"Standard\"\n",
    "\"Diviation\" and \"Variance\"\n",
    "\n",
    "    \n",
    "per suppose i have dataset={1,1,2,2,4} = 10/5 = 2 mean\n",
    "                            {2,2,2,2,2} = 10/5 = 2 \"mean\" for this\n",
    "                                    both distribution we r getting \n",
    "                                    same average or \"mean\" so how do\n",
    "                                    i identify the this 2 distributions\n",
    "                                    are different. \n",
    "                    \n",
    " NOTE := if u really identify how 2 distributions r defferent that \n",
    "         point of time we may use variance and standard diviation\n",
    "        \n",
    "  @@@* what is \"Variance\" *@@@\n",
    "    @@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "Variance bassically says that  how well/goodly your \"DATA is Spread\"\n",
    "again we define 2 things.\n",
    "\n",
    "Note : with the help of \"Variance\" you will be able to\n",
    "       understand how the \"data spreadness\" is happens,\n",
    "        \n",
    "       with 'standard diviation(SD)' u will be able to \n",
    "       understand that B/W 1 SD to the right & to the left \n",
    "       what may be the range of data that will be folling. \n",
    "    \n",
    "\n",
    "Population_Variance (σ²)           Sample_Variance(s²)\n",
    "--------------------------   |   --------------------------\n",
    "                             |\n",
    "Formula for Population       |       Formula for Sample  \n",
    "Variance is                  |        Variance is\n",
    "                             |\n",
    "      N   (xi-µ)²            |            n   (xi-x̄)²\n",
    "σ² =  ∑   ------             |      s² =  ∑   ------\n",
    "     i=1     N               |           i=1    n-1    \n",
    "                             |         \n",
    "Here \"µ\" is \"Population mean\"|      Here \"x̄\" is the \"Sample mean\"\n",
    "                             |\n",
    "--------------------------------------------------------------    \n",
    "ex:=                    \n",
    "        \n",
    " X = {1,2,2,3,4,5}  \n",
    "\n",
    "mean \" µ \" = 2.83\n",
    "\n",
    "              µ        (xi-µ)    (xi-µ)²\n",
    "             --------------------------- \n",
    "   x          x̄        (xi-x̄)    (xi-x̄)²\n",
    "   1         2.83      -1.83      3.3489\n",
    "   2         2.83      -0.83      0.6889  \n",
    "   2         2.83      -0.83      0.6889\n",
    "   3         2.83       0.17      0.0289\n",
    "   4         2.83       1.17      1.3689\n",
    "   5         2.83       2.17      4.7089  \n",
    "                                ----------\n",
    "                                  10.84\n",
    "              \n",
    "                           N   (xi-µ)²   10.84 \n",
    "Population Variance (σ²) = ∑   ------  = -----  = 1.8066     \n",
    "                          i=1     N        6\n",
    "        \n",
    "                       n   (xi-x̄)²   10.84\n",
    "Sample Variance (s²) = ∑  ------- = ------- = 2.168\n",
    "                      i=1    n-1       5   \n",
    "    \n",
    "why n-1 means ?\n",
    "    \n",
    "(n-1)its called 'basal correction' we also it as 'Degree of freedom '\n",
    "       \n",
    "            \n",
    "note : if your \"Variance is higher\" then your Graph{bell curv}\n",
    "      \"spredness is also Higher\"                \n",
    "                    \n",
    "    just concider \"variance\" like 6.86 so in this case but our actual \n",
    "    \"variance\" is 2.168, so that your graph spreadness is also higher \n",
    "    \n",
    "                                                 *\n",
    "                                               *   *   \n",
    "                       \" µ \" = 2.83            *    * \n",
    "                                               *    *\n",
    "          *  *  *                              *     *  \n",
    "       *           *                          *       *\n",
    "     *                *                      *         *  \n",
    "   *                     *                 *             *\n",
    "  *    SD = 6.86            *             *    Sd = 1.47    *\n",
    " *                          *          *                       * \n",
    "---------------|----------------   ---------------|----------------\n",
    "              2.83                              2.83           \n",
    "            \n",
    "    \n",
    "       @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@             \n",
    "  @@@*  what is  Standard diviation = {σ} *@@@    \n",
    "       @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    \n",
    "    Formula :=\n",
    "\n",
    " \"Standard deviation \" SD (σ)  = √ variance  = √2.168 = 1.47 \n",
    "    \n",
    "    { 1,2,2,3,4,5 }\n",
    "    \n",
    "    mean \" µ \" = 2.83 add it 1.47 it will be your 1st SD \n",
    "                 if your minus it  1st -LSD if i constract\n",
    "                 the graph its looks like \"Bell curve\"\n",
    "            \n",
    "            2.83+1.47 = 4.302\n",
    "            4.302+1.47 =5.774 ---to right\n",
    "            \n",
    "            2.83-1.47=1.358\n",
    "            1.358-1.47=-0.114\n",
    "            \n",
    "               *\n",
    "            |------|\n",
    "           2.83   4.302 means the elements which folling in this regoin \n",
    "           \"mean\"  \n",
    "                            *  * \n",
    "                        *         *\n",
    "                      *              *\n",
    "                    *                  *    \n",
    "                   *                     *\n",
    "                 *                         *         \n",
    "                *                            *\n",
    "               *                              *\n",
    "             *                                 *\n",
    "           *                                     *\n",
    "outliers * 3SD  2LSD  1LSD       1SD    2SD  3SD  *   outliers\n",
    "        -|---|------|---------|------|------|-----|---\n",
    "         -0.114   1.358     2.83   4.302  5.774  7.246\n",
    "                         \"mean\" \n",
    "            \n",
    " why this is called \"Bell curve\"  ?\n",
    "Ans:=\n",
    "    maximum No.of elements B/W 2nd SD to the left and to the right\n",
    "    will be folling, then as the No.of elements is getting reduced\n",
    "    this curve is going come down. \n",
    "    \n",
    "    then we learn about \"gousion distribution\" why this curv falling down.\n",
    "    \"gousion distribution\" looking like this we something called as \n",
    "    \"empirical formula\"\n",
    "    \n",
    "    at the \"Bell curv\" down elements fall as \"Outliers\" this is what we \n",
    "    do in \"Infrential statics\"\n",
    "\n",
    "3#####################################################################################\n",
    "######################################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1cf4117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.25"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-4.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42380a63",
   "metadata": {},
   "source": [
    "# Percentiles And Quatiles Removing the Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe52c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@@@@@@@@@@@@@@@@@@@@@@@@@       \n",
    "Percentiles And Quatiles :=\n",
    "@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    " \"this is the 1st step to find outliers\"\n",
    "\n",
    "    Percentages :=\n",
    "        1,2,3,4,5   = odd ele 1,3,5\n",
    "        \n",
    "        what % of the numbers that are odd?\n",
    "        \n",
    "                                 3\n",
    "        % no.of odd/total.no  = --- =0.6 = 60%\n",
    "                                 5\n",
    "            \n",
    "Percentiles :=\n",
    "    A \"Percentiles\" is a value bellow which a \"certain percentage\" of\n",
    "    observation lie\n",
    "        \n",
    " Exaple :-\n",
    "        the person has got better mark then the 99% of the student\n",
    "        \n",
    "        {cat ,sat ,gate } Based on Percentiles \n",
    "        \n",
    "why we use percentiles in statistics ?\n",
    " 1st is 'ranking'\n",
    "        \n",
    "Data ={2,2,3,4,5,5,5,6,7,8,8,8,8,8,9,9,10,11,11,12} its already sorted\n",
    "        \n",
    "        {Q} what is the \"Percentiles\" ranking of 10 ?  n=20\n",
    "        \n",
    "        Ans :=    x = 10\n",
    "            \n",
    "                         no.of value below x           16\n",
    "Percentiles rank of X = --------------------- x 100 = ----x100 = 0.8x 100 = 80%\n",
    "                                   n                   20 \n",
    "    \n",
    " Note:= meaning of this 80% is, 10 is indicates that its  80% grater then the \n",
    "       entire distribution  of the dataset\n",
    "    \n",
    "        {Q} what is the Percentiles ranking of 11 ?  n=20\n",
    "          Ans := \n",
    "                \n",
    "                        no.of value below X           17\n",
    "Percentiles rank of X= --------------------- x 100 = ----x100 = 0.85x 100 = 85%\n",
    "                                  n                   20\n",
    "    \n",
    " Note:= meaning of this 85% is, 11 is indicates that its  85% grater then the \n",
    "         entire distribution  of the dataset.\n",
    "    \n",
    "           \n",
    "        {Q} what value \"exist\" at percentile ranking of 25%\n",
    "        \n",
    "                Pecentile             25\n",
    "        Value = ---------- x (n+1) = ----- x (21) = 5.25 this is 'Index' of our dataset\n",
    "                   100                100\n",
    "            \n",
    "            5+5   10\n",
    "            --- = --- = 5\n",
    "             2     2\n",
    "            \n",
    "           the index of ANS:  5 is the Value for 25% \n",
    "            \n",
    "            \n",
    "          {Q} what value exist at percentile ranking of 75%\n",
    "        \n",
    "                percetaile             75\n",
    "        Value = ----------- x (n+1) = -----x(21) = 15.75 this is 'index position' of dataset\n",
    "                   100                 100\n",
    "            \n",
    "              9+9\n",
    "              ---- = 9 vaule exist at percentile ranking of 75%\n",
    "               2 \n",
    " Note:= meaning of this 75% is 75% of the entire distribution is \n",
    "       less then 9             \n",
    "                \n",
    "            \n",
    "{very IMP }\n",
    "\n",
    "$$$$$$$$$$$$$$$$$$$$$$\n",
    "Five Number Summary :=\n",
    "$$$$$$$$$$$$$$$$$$$$$$\n",
    "\n",
    "    inside this we need to understand\n",
    "      1. Minimum\n",
    "      2. First Quatile.Q1.(25%) --(Q1)\n",
    "      3. Median\n",
    "      4. Third Quatile.Q3.(75%) --(Q3) \n",
    "      5  Maximum \n",
    "        \n",
    "What is Quatiles ?\n",
    "\n",
    "Ans:=\n",
    "    Quatile means 25%,75%,50% percentile\n",
    "        \n",
    "        \n",
    "{IMP}\n",
    "Removing the Outliers :=\n",
    "    \n",
    "    Data_set ={1,2,2,2,3,3,4,5,5,5,6,6,6,6,7,8,8,9,27}\n",
    "    by seeing 'outlier' is 27 \n",
    "      \n",
    "    in coming to removing the outliers the 1st step is \n",
    "    that we really need to define our *Fences\n",
    "    \n",
    "    NOTE:\n",
    "    we should try to find out a value' Lower fence' nd 'Higher Fence'\n",
    "    b/w which if all the elements are present will concider it.\n",
    "    if any element 'away from this fence' will concider as a 'outliers'\n",
    "      \n",
    "       1.Lower Fence\n",
    "       2.Higer Fence\n",
    "            \n",
    "       *Lower Fence :=\n",
    "               Lower Fence = Q1-1.5(IQR)\n",
    "            \n",
    "           Lower Fence is nothing but ---> Q1-1.5(IQR)\n",
    "           IQR is nothing but 'Inter Quatile Range'\n",
    "           'Inter Quatile Range' nothing but your \n",
    "           'subtracting' ( 75% - 25% ) like (Q3-Q1)\n",
    "            \n",
    "            the difference B/W Q3 and Q1 is called IQR\n",
    "            'inter quatile range'\n",
    "            \n",
    "            whaat is Q3 or 25th% value ?\n",
    "            \n",
    "               data n =19 numbers\n",
    "        \n",
    "                Pecentile            25\n",
    "        Value = ----------x (n+1) = ----- x (20) = 5 this is 'Index' of our dataset\n",
    "                   100               100 \n",
    "            \n",
    "            similarly 75% value = 15  this is indicate 'index'\n",
    "                     \n",
    "                so Q1 = 3    Q3  =  7\n",
    "            \n",
    "        *Higer Fence :=\n",
    "              Higer Fence =  Q3+1.5(IQR)\n",
    "                \n",
    "             Higer Fence can be defined as-->Q3+1.5(IQR) \n",
    "            \n",
    "            IQR = (Q3-Q1)\n",
    "            \n",
    "            \n",
    "            \n",
    "          here for the data Q1 index = 5 , Q3 index = 15 \n",
    "               \n",
    "             Q1 = 3    Q3 =  7   \n",
    "                \n",
    "            IQR = Q3 - Q1 = 7-3 = 4\n",
    "        \n",
    "        Lower Fence  = 3 - 1.5(4) = - 3\n",
    "        Higher Fence = 7 + 1.5(4) = 13\n",
    "        \n",
    "        [ \"Lowe Fence\"  <-----> \"Higher Fence\" ]\n",
    "               [   -3 <-----> 13    ]\n",
    "            \n",
    "            \n",
    "here removing \"Outliers\" which ever not folling with in -3 to 13/\n",
    "\"lower fence\" and \"Higher Fence\" that will be removed simply which are\n",
    "'less then -3' and 'greter then 13' those will be removed in this problem stmt\n",
    "    \n",
    "    Data_set ={1,2,2,2,3,3,4,5,5,5,6,6,6,6,7,8,8,9,27}\n",
    "            \n",
    "        27 > 13\n",
    "        \n",
    "        after removing \"outliers\" remained numebers\n",
    "            \n",
    "    data = {1,2,2,2,3,3,4,5,5,5,6,6,6,6,7,8,8,9}\n",
    "    \n",
    "[27>13] 27 is \"greater then\" 13 so we  remove the outlier 27 after \n",
    "removing outliers come to 5 number summary\n",
    "        \n",
    "              Minimum = 1\n",
    "              Q1 = 3\n",
    "              Median  = 5  \n",
    "              Q3 = 7  \n",
    "              max     = 9  by this specific data u\n",
    "                           will draw box plot\n",
    "                    \n",
    "                    calculae median othen way = 50%/100(n+1)=9.5 index \n",
    "                     5+5/2 = 10/2 = 5\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "\"Box Plot\" :=\n",
    "    \n",
    "    box plot is use to see outliers\n",
    "    \n",
    "    \n",
    "                    Q1.3  mid.5  Q3.7 \n",
    "             min 1    -------------    9 max\n",
    "                |     |     |     |     |\n",
    "                |-----|     |     |-----|\n",
    "                |     |     |     |     |\n",
    "                      -------------\n",
    "   <-----|---|-----|-----|-----|-----|-----|-----|----->\n",
    "        -2   0     2     4     6     8     10   12\n",
    "    \n",
    "if we not delete outlier 27 value it shows you lke ...\n",
    "\n",
    "\n",
    "                    Q1.3  mid.5  Q3.7 \n",
    "             min 1    -------------    9 max\n",
    "                |     |     |     |     |\n",
    "                |-----|     |     |-----|                   🔴outlier\n",
    "                |     |     |     |     | \n",
    "                      -------------\n",
    "   <-----|---|-----|-----|-----|-----|-----|-----|-----------|-->\n",
    "        -2   0     2     4     6     8     10   12           27 \n",
    "    \n",
    "    \n",
    "use of 'Box plot' it is \"visuvalization\" way to see the \"Outlier\"\n",
    "it basically use where an \"Outlier\" is actually present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b52038",
   "metadata": {},
   "source": [
    "# Standardization  and  Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d3e819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d65d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={\"class\":[1,1,1,1,1],\"alcohol\":[14.23,13.20,13.16,14.37,13.24],\"malic\":[1.71,1.78,2.36,1.95,2.59]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfcedd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class  alcohol  malic\n",
       "0      1    14.23   1.71\n",
       "1      1    13.20   1.78\n",
       "2      1    13.16   2.36\n",
       "3      1    14.37   1.95\n",
       "4      1    13.24   2.59"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(d)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2889e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804445cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3343ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88429752, 0.        ],\n",
       "       [0.03305785, 0.07954545],\n",
       "       [0.        , 0.73863636],\n",
       "       [1.        , 0.27272727],\n",
       "       [0.0661157 , 1.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaling.fit_transform(df[['alcohol','malic']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbd21d2",
   "metadata": {},
   "source": [
    "# Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21acea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11c43aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b09ab56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.08997918, -1.07836773],\n",
       "       [-0.81286583, -0.87324344],\n",
       "       [-0.88676272,  0.82635788],\n",
       "       [ 1.3486183 , -0.37508443],\n",
       "       [-0.73896893,  1.50033771]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaling.fit_transform(df[['alcohol','malic']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8650ac",
   "metadata": {},
   "outputs": [],
   "source": [
    " -3.2 ,-2.2,-0.2,1.8,3.8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c5a550b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5599999999999987"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-10.240000000000002)+(-4.840000000000001)+(-0.04000000000000001)+(3.24)+(14.44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c78ee26d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=((-3.2)**2)+((-2.2)**2)+((-0.2)**2)+((1.8)**2)+((3.8)**2)\n",
    "\n",
    "32.8/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32ca6473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.552713678800501e-15"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(25-28.2)+(26-28.2)+(28-28.2)+(30-28.2)+(32-28.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9aeae7ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.24066143273463e-15"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.552713678800501e-15/2.863564212655271"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc58a9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905a43e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e98a399",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= {\"Age\":[25,26,28,30,32],\n",
    "      \"Weight\":[65,53,73,60,70],\n",
    "      \"Salary\":[30,40,60,50,80]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85afe746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1c6e32e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "      <td>65</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>53</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>73</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age  Weight  Salary\n",
       "0   25      65      30\n",
       "1   26      53      40\n",
       "2   28      73      60\n",
       "3   30      60      50\n",
       "4   32      70      80"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec00c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c299d090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaling=StandardScaler()\n",
    "scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1921ed0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=scaling.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c91f63b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4802973661668754e-17"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2e3bf224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2493901 ,  0.11206636, -1.27872403],\n",
       "       [-0.85895569, -1.56892908, -0.69748583],\n",
       "       [-0.07808688,  1.23272999,  0.46499055],\n",
       "       [ 0.70278193, -0.58834841, -0.11624764],\n",
       "       [ 1.48365074,  0.81248113,  1.62746694]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8ae09635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age\n",
       "0   25\n",
       "1   26\n",
       "2   28\n",
       "3   30\n",
       "4   32"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= {\"Age\":[25,26,28,30,32]}\n",
    "dd=pd.DataFrame(data)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "01da5828",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da0bb61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2493901 ],\n",
       "       [-0.85895569],\n",
       "       [-0.07808688],\n",
       "       [ 0.70278193],\n",
       "       [ 1.48365074]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale.fit_transform(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ae258548",
   "metadata": {},
   "outputs": [],
   "source": [
    "data={\"f1\":[2,5,6,8,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1a15d1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   f1\n",
       "0   2\n",
       "1   5\n",
       "2   6\n",
       "3   8\n",
       "4   1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1=pd.DataFrame(data)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "acbfed1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d989c309",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "975407f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14285714],\n",
       "       [0.57142857],\n",
       "       [0.71428571],\n",
       "       [1.        ],\n",
       "       [0.        ]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale.fit_transform(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ebc949fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale.feature_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2dad9006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale.fit(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e9e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
